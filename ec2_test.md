## AWS EC2 ì¸ìŠ¤í„´ìŠ¤ í¬ê¸°ì— ë”°ë¥¸ ë¹„êµ


> ðŸ’¡ **ê³µí†µ ì¡°ê±´**
- **ì•„í‚¤í…ì²˜** : ARM
- **ì¸ìŠ¤í„´ìŠ¤** **íŒ¨ë°€ë¦¬** : t
- **ì¸ìŠ¤í„´ìŠ¤** **ì„¸ëŒ€** : 4
- **ìŠ¤í† ë¦¬ì§€** : gp3, 80GiB

- **ì¸ìŠ¤í„´ìŠ¤** **í¬ê¸°** : nano, small, micro


> ðŸ’¡ **ì‹¤í—˜ ë‚´ìš©**

- ì¸ìŠ¤í„´ìŠ¤ í¬ê¸°ë§Œì„ ë‹¤ë¥´ê²Œ ì„¤ì •í•œ ë’¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- ê° ì¸ìŠ¤í„´ìŠ¤ì— Anaconda ì„¤ì¹˜ í›„ ê°€ìƒ í™˜ê²½ ìƒì„±
- ê°€ìƒ í™˜ê²½ì—ì„œ ìˆ˜ê°•ìƒì˜ Github íŽ˜ì´ì§€ë¥¼ Crawlingí•˜ëŠ” python ì½”ë“œ ì‹¤í–‰
    - Crawling Code
- 5ë²ˆì”© ì‹¤í–‰ í›„ í‰ê·  ê³„ì‚°
        
        ```python
        import requests
        from bs4 import BeautifulSoup
        import os
        import time
        
        start = time.time()
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ URL í•œì¤„ì”© ì½ê¸°
        with open("github_profile.txt", "r") as f:
            urls = f.readlines()
        
        # ê° URLë§ˆë‹¤ ì •ë³´ ì½ê¸°
        for url in urls:
            url = url.strip() # ì¤„ë°”ê¿ˆ ì œê±°
            response = requests.get(url)
        
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
        
            name = url.split("github.io/")[-1][:-1]
        
            with open(os.path.join('github', f"{name}.txt"), "w", encoding='utf-8') as f:
                f.write(text)
                f.close()
                print(name + "ìˆ˜ì§‘/ì €ìž¥ ì™„ë£Œ")
        end = time.time()
        
        print(f"{end - start} ì´ˆ")
        ```
        

> ðŸ’¡ **ì‹¤í—˜ ê²°ê³¼**

- t4.nano : 2.0314 sec
- t4.micro : 2.078 sec
- t4.small : 2.1866 sec
